<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Open-Source on Adi Polak</title><link>https://blog.adipolak.com/tags/open-source/</link><description>Recent content in Open-Source on Adi Polak</description><generator>Hugo</generator><language>en-us</language><copyright>Copyright &amp;#169; 2020 Adi Polak. All rights reserved.</copyright><lastBuildDate>Mon, 22 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.adipolak.com/tags/open-source/index.xml" rel="self" type="application/rss+xml"/><item><title>Delta Lake essential Fundamentals: Part 4 - Practical Scenarios</title><link>https://blog.adipolak.com/post/delta-lake-essential-fundamentals---part-4/</link><pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.adipolak.com/post/delta-lake-essential-fundamentals---part-4/</guid><description>&lt;p>ðŸŽ‰ Welcome to the 4th part of Delta Lake essential fundamentals: the practical scenarios! ðŸŽ‰&lt;/p>
&lt;p>There are many great features that you can leverage in delta lake, from the ACID transaction, Schema Enforcement, Time Traveling, Exactly One semantic, and more.&lt;/p>
&lt;p>Let&amp;rsquo;s discuss two common data pipelines patterns and solutions:&lt;/p>
&lt;h2 id="spark-structured-streaming-etl-with-deltalake-that-serves-multiple-users">Spark Structured Streaming ETL with DeltaLake that serves multiple Users&lt;/h2>
&lt;p>&lt;strong>Spark Structured Streaming&lt;/strong>-
Apache Spark structured steaming are essentially unbounded tables of information. There is a continuous stream of data ingested into the system. As developers, we write the code to process the data continuously.
&lt;strong>ETL&lt;/strong> stands for &lt;strong>E&lt;/strong>xtract, &lt;strong>T&lt;/strong>ransform and &lt;strong>L&lt;/strong>oad.&lt;/p></description></item><item><title>Delta Lake essential Fundamentals: Part 3 - compaction and checkpoint</title><link>https://blog.adipolak.com/post/delta-lake-essential-fundamentals---part-3/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.adipolak.com/post/delta-lake-essential-fundamentals---part-3/</guid><description>&lt;p>Let&amp;rsquo;s understand what are Delta Lake compact and checkpoint and why they are important.&lt;/p>
&lt;h2 id="checkpoint">Checkpoint&lt;/h2>
&lt;p>There are two known checkpoints mechanism in Apache Spark that can confuse us with DeltaLake checkpoint, so let&amp;rsquo;s understand them and how they differ from each other:&lt;/p>
&lt;h3 id="spark-rdd-checkpoint">Spark RDD Checkpoint&lt;/h3>
&lt;p>Checkpoint in Spark RDD is a mechanism to persist current RDD to a file in a dedicated checkpoint directory while all references to its parent RDDs are removed.
This operation, by default, breaks data lineage when used without auditing.&lt;/p></description></item><item><title>Delta Lake essential Fundamentals: Part 2 - The DeltaLog</title><link>https://blog.adipolak.com/post/delta-lake-essential-fundamentals---the-deltalog/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.adipolak.com/post/delta-lake-essential-fundamentals---the-deltalog/</guid><description>&lt;p>In the previous part, you learned what &lt;a href="https://blog.adipolak.com/post/delta-lake-essential-fundamentals">ACID transactions&lt;/a> are.&lt;br>
In this part, you will understand how Delta Transaction Log, named DeltaLog, is achieving ACID.&lt;/p>
&lt;h2 id="transaction-log">Transaction Log&lt;/h2>
&lt;p>A transaction log is a history of actions executed by a (TaDa ðŸ’¡) database management system with the goal to guarantee &lt;a href="https://blog.adipolak.com/post/delta-lake-essential-fundamentals/">ACID properties&lt;/a> over a crash.&lt;/p>
&lt;h2 id="deltalake-transaction-log---detlalog">DeltaLake transaction log - DetlaLog&lt;/h2>
&lt;p>DeltaLog is a transaction log directory that holds an &lt;strong>ordered&lt;/strong> record of every transaction committed on a Delta Lake table since it was created.
The goal of DeltaLog is to be the &lt;strong>single&lt;/strong> source of truth for readers who read from the same table at the same time. That means, parallel readers read the &lt;strong>exact&lt;/strong> same data.
This is achieved by tracking all the changes that users do: read, delete, update, etc. in the DeltaLog.&lt;/p></description></item><item><title>Delta Lake essential Fundamentals: Part 1 - ACID</title><link>https://blog.adipolak.com/post/delta-lake-essential-fundamentals/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.adipolak.com/post/delta-lake-essential-fundamentals/</guid><description>&lt;p>ðŸŽ‰ Welcome to the first part of Delta Lake essential fundamentals! ðŸŽ‰&lt;/p>
&lt;h2 id="what-is-delta-lake-">What is Delta Lake ?&lt;/h2>
&lt;blockquote>
&lt;p>Delta Lake is an open-source storage layer that brings ACID
transactions to Apache Sparkâ„¢ and big data workloads. &lt;/p>
&lt;/blockquote>
&lt;p>DeltaLake open source consists of 3 projects:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://github.com/delta-io/delta">detla&lt;/a> - Delta Lake core, written in Scala.&lt;/li>
&lt;li>&lt;a href="https://github.com/delta-io/delta-rs">delta-rs&lt;/a> - Rust library for binding with Python and Ruby.&lt;/li>
&lt;li>&lt;a href="https://github.com/delta-io/connectors">connectors&lt;/a> - Connectors to popular big data engines outside Spark, written mostly in Scala.&lt;/li>
&lt;/ol>
&lt;p>Delta provides us the ability to &lt;u>&amp;ldquo;travel back in time&amp;rdquo;&lt;/u> into previous versions of our data, &lt;u>scalable metadata&lt;/u> - that means if we have a large set of raw data stored in a data lake, having metadata provides us with the flexibility needed for analytics and exploration of the data. It also provides a mechanism to &lt;u>unify streaming and batch data&lt;/u>.&lt;br>
&lt;u>Schema enforcement&lt;/u> - handle schema variations to prevent insertion of bad/non-compliant records, and &lt;u>ACID transactions&lt;/u> to ensure that the users/readers never see inconsistent data.&lt;/p></description></item><item><title>Apache Spark Ecosystem, Jan 2021 Highlights</title><link>https://blog.adipolak.com/post/apache-spark-ecosystem/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://blog.adipolak.com/post/apache-spark-ecosystem/</guid><description>&lt;p>If you&amp;rsquo;ve been reading here for a while, you know that I&amp;rsquo;m a big fan of Apache Spark and have been using it for more than 8 years.&lt;br>
Apache Spark is continually growing. It started as part of the Hadoop family,&lt;br>
but with &lt;a href="https://medium.com/@acmurthy/hadoop-is-dead-long-live-hadoop-f22069b264ac">the slow death of hadoop&lt;/a> and the fast growth of Kubernetes, many new tools, connectors and open source have emerged.&lt;/p>
&lt;p>Let&amp;rsquo;s take a look at three exciting open sources:&lt;/p></description></item></channel></rss>