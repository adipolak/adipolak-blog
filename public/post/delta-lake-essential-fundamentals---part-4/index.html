<!DOCTYPE html>
<html lang="en-us">
  <head>
    <link rel="stylesheet" href="/css/custom.css">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Adi Polak  | Delta Lake essential Fundamentals: Part 4 - Practical Scenarios</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.140.2">
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    <link rel="shortcut icon" type="image/png" href="/assets/favicon/favicon-32x32.png"/>

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:url" content="https://blog.adipolak.com/post/delta-lake-essential-fundamentals---part-4/">
  <meta property="og:site_name" content="Adi Polak">
  <meta property="og:title" content="Delta Lake essential Fundamentals: Part 4 - Practical Scenarios">
  <meta property="og:description" content="Multi-part series that will take you from beginner to expert in Delta Lake">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2021-02-22T00:00:00+00:00">
    <meta property="article:modified_time" content="2021-02-22T00:00:00+00:00">
    <meta property="article:tag" content="Open-Source">
    <meta property="article:tag" content="Apache Spark">
    <meta property="article:tag" content="Delta Lake">
    <meta property="article:tag" content="Beginner">
    <meta property="article:tag" content="Scenarios">

  <meta itemprop="name" content="Delta Lake essential Fundamentals: Part 4 - Practical Scenarios">
  <meta itemprop="description" content="Multi-part series that will take you from beginner to expert in Delta Lake">
  <meta itemprop="datePublished" content="2021-02-22T00:00:00+00:00">
  <meta itemprop="dateModified" content="2021-02-22T00:00:00+00:00">
  <meta itemprop="wordCount" content="1115">
  <meta itemprop="keywords" content="Open-Source,Apache Spark,Delta Lake,Beginner,Scenarios">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Delta Lake essential Fundamentals: Part 4 - Practical Scenarios">
  <meta name="twitter:description" content="Multi-part series that will take you from beginner to expert in Delta Lake">

  </head>

  <body class="ma0 avenir bg-near-white">

    
  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://blog.adipolak.com/" class="f3 fw2 hover-white no-underline white-90 dib header-logo-wrapper">
      
      
      
      
      <img src="/assets/favicon/favicon-32x32.png" class="header-logo" />
      Adi Polak
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3 header-nav">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90 header-nav-link" href="/about/" title="about page">
              about
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90 header-nav-link" href="/presentations/" title="presentations page">
              presentations
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90 header-nav-link" href="/subscribe/" title="subscribe page">
              subscribe
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90 header-nav-link" href="/contact-me/" title="contact page">
              contact
            </a>
          </li>
          
        </ul>
      
      











    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        
        <h2 class="f2 f-subheadline-s fw2 white-90 mb0 lh-title">
          <span class="header-subtitle-inner">distributed systems, AI/ML, and bold tech opinions</span>
        </h2>
        
      </div>
    </div>
  </header>


    <main role="main">
      
<div class="layout mw8 center ph3 flex-l">
  
  

  
  <div class="content flex-auto">
    <article class="flex-l flex-wrap justify-between">
      <header class="mt4 w-100">
        <aside class="instapaper_ignoref b helvetica tracked ttu">
          Posts
        </aside>
        <h1 class="f1 athelas mt3 mb1">Delta Lake essential Fundamentals: Part 4 - Practical Scenarios</h1>
        
        <p class="tracked">
           <strong>Adi Polak
          </strong>
        </p>
        
        
        <time class="f6 mv4 dib tracked" datetime="2021-02-22T00:00:00Z">February 22, 2021</time>
        
      </header>

      <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l"><p>üéâ Welcome to the 4th part of Delta Lake essential fundamentals: the practical scenarios! üéâ</p>
<p>There are many great features that you can leverage in delta lake, from the ACID transaction, Schema Enforcement, Time Traveling, Exactly One semantic, and more.</p>
<p>Let&rsquo;s discuss two common data pipelines patterns and solutions:</p>
<h2 id="spark-structured-streaming-etl-with-deltalake-that-serves-multiple-users">Spark Structured Streaming ETL with DeltaLake that serves multiple Users</h2>
<p><strong>Spark Structured Streaming</strong>-
Apache Spark structured steaming are essentially unbounded tables of information. There is a continuous stream of data ingested into the system. As developers, we write the code to process the data continuously.
<strong>ETL</strong> stands for <strong>E</strong>xtract, <strong>T</strong>ransform and <strong>L</strong>oad.</p>
<p><strong>Scenario</strong> - Ingest data from Kafka topic, process the information using Spark structured streaming, and save it to DeltaLake for multiple <em>users</em> on-the-fly queries. <br>
Note! The output of the solution/pipeline is used by real users and not a machine, which means that there is no need to refresh the data every couple of seconds as the person taking actions on the data won&rsquo;t be able to use it.</p>
<h3 id="system-requirements">System Requirements</h3>
<p>Let&rsquo;s assume we have these system requirements: <br><br>
<strong>Input:</strong> any unstructured one input stream for example Kafka topic <br>
<strong>Output:</strong> structured tabular data for users to query <br>
<strong>Latency:</strong> 5 minutes <br>
<strong>Constraints:</strong> multiple users query the table at the same time</p>
<h3 id="high-level-pipeline-architecture">High-level Pipeline Architecture</h3>
<img class="responsive" src="/images/Detla/kafka-spark-streaming-delta-scenario.png" alt="drawing">
<h3 id="advantages">Advantages</h3>
<p>In our scenario, we have multiple users that query the data on the fly and should see the same data - <em>single source of truth</em>. In distributed steaming, when we query the data, it might be that from two identical queries that ran at the same time, we will get different results. This is why we introduce DeltaLake into the pipeline. We save the streaming tabular data in DeltaLake, which in practice means that the user read operations take place on the DeltaTable snapshot, which guarantees consistency of the data. At the same time, the table is continuously being written.<br>
Simultaneously, the user can also run updates/deletes and fixes on the data when necessary, this is important for controlling incoming data that are bounded to GDPR or other compliances. The conflicts are resolved using Delta conflict resolution mechanism (Discussed in Part 2 - <a href="https://blog.adipolak.com/post/delta-lake-essential-fundamentals-the-deltalog/">the DeltaLog</a>).</p>
<p>If you are using <a href="https://docs.microsoft.com/en-us/azure/databricks/delta/?WT.mc_id=delta-13569-adpolak">Databricks services</a>, you will get the Auto Optimize out of the box, which coalesces small files into larger files using <a href="https://docs.microsoft.com/en-us/azure/databricks/delta/optimizations/auto-optimize?WT.mc_id=delta-13569-adpolak">Auto Compaction</a>.</p>
<h3 id="when-to-exclude-delta-lake">When to exclude Delta Lake</h3>
<p>As much as it&rsquo;s important to know what are the advantages and when to use DeltaLake, it&rsquo;s important to understand when to exclude it. For example, when you want to have a latency of <strong>seconds</strong> to update a Key-Value output for lookup tables, you should probably avoid DeltaLake since it introduce the overhead of the optimistic concurrency and commits to the DeltaLog itself. But if your system can handle a couple minutes of latency, you should consider using it for enforcing data - <em>single source of truth</em> for your users.</p>
<h4 id="what-to-use-instead-of-deltalake-for-updating-lookup-tables-with-seconds-latency">What to use instead of DeltaLake for updating lookup tables with seconds latency</h4>
<p>A lookup table is an array that replaces runtime computation with a simpler array indexing operation, which means that the data is being stored in memory. This makes read queries significantly faster than loading data from disk, which involves I/O operations. Hence for stateful streaming operations, we would prefer to use in-memory databases such as <a href="https://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-overview?WT.mc_id=delta-13569-adpolak">Redis</a>, <a href="https://docs.microsoft.com/en-us/azure/cosmos-db/cassandra-introduction?WT.mc_id=delta-13569-adpolak">Cassandra</a> or Amazon DynamoDB.
<strong>This solution is more expensive</strong> since it requires dedicated servers/services to be used in the system, vs. using DeltaLake which is a storage layer, but this is the price to be paid for lookup table that is being updated and accurate to all users with a latency of seconds.</p>
<!-- <highlight>
<p style="font-family:verdana;" >Note: the DetlaLake merge capabilities are currently supported in Databricks environment but not yet in the OSS.
</p>
</highlight> -->
<hr>
<p><br> <br></p>
<h2 id="join-multiple-data-streams-based-on-a-common-key-on-azure-databricks">Join Multiple Data Streams based on a common key on Azure Databricks</h2>
<p><strong>Scenario</strong> -  Ingesting data into the system from multiple different data streams that need to be joined based on a common key written to a shared table for future analytics/ML workloads/lookup tables.</p>
<h3 id="system-requirements-1">System Requirements</h3>
<p>Let&rsquo;s assume we have these system requirements: <br><br>
<strong>Input:</strong> multiple unstructured input streams from various sources <br>
<strong>Output:</strong> tabular data combining the streams inputs <br>
<strong>Latency:</strong> 2 minutes <br>
<strong>Constraints:</strong> support join on one fast and one slow data streams with dimension changes</p>
<p>Slowly changing dimensions is a data management problem where the data warehouse contains relatively static data and schema linked to a dimension table that can change its schema and data as time passes. To learn more about it, read <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension">here</a>. Imagine a user bank account information that needs to be joined by a user bank transaction.</p>
<p>Suppose you are familiar with <a href="https://docs.microsoft.com/en-us/azure/databricks/kb/sql/bchashjoin-exceeds-bcjointhreshold-oom?WT.mc_id=delta-13569-adpolak">broadcast join mechanism</a>. In that case, the solution might look simple, broadcast the small static data table and use Spark Structured Streaming to stream the fast, ever-changing stream for the join operation.<br>
But, what if you are attempting to join two big tables that constantly change? This is when you need to understand how to leverage Delta Lake versioning capabilities.</p>
<h3 id="high-level-pipeline-architecture-1">High-level Pipeline Architecture</h3>
<p>In this high-level architecture diagram, we have 2 Spark workloads; the first one is a batch, reading data from MongoDB, processing it, and saving it to DetlaLake. The second one is the fast data, ingesting data from a Kafka topic directly into Spark Streaming and joining the fast data with the slow data saved in DeltaLake. After the join and further logic, the data is being kept in a tabular store for future use.</p>
<img class="responsive" src="/images/Detla/azure-databricks-streaming-with-deltalake.jpg" alt="drawing">
<h3 id="advantages-1">Advantages</h3>
<p>Without DetlaTable, Structured streaming will hold a static view of the first &ldquo;slow&rdquo; data, and it won&rsquo;t be updated until you restart the streaming query. But we already know that this data can be updated; think about the bank customer who changed their home address, and it needs to be updated in the system.
Using DeltaLake helps introduce table versioning and allows you to control the changes and read/use the latest/most updated version inside the streaming query without restarting it.</p>
<highlight>
<p style="font-family:verdana;" > Note: the DetlaLake automatic reload without restart capabilities are currently supported in Databricks environment but not yet in the OSS.
</p>
</highlight>
<hr>
<iframe src="https://giphy.com/embed/Ec5RkrmARxPmTuXgrZ" width="480" height="360" frameBorder="0" class="responsive" allowFullScreen></iframe>
<p>I hope you enjoyed reading about Delta Lake, the two practical scenarios, and the breakdown of the open-source;  <br>
I will continue to share scenarios, insights, and code samples throughout the blog. As always, if you have questions, suggestions, ideas, please don&rsquo;t hesitate to DM me on <a href="https://twitter.com/intent/follow?original_referer=http%3A%2F%2Flocalhost%3A1313%2F&amp;ref_src=twsrc%5Etfw&amp;region=follow_link&amp;screen_name=AdiPolak&amp;tw_p=followbutton">Adi Polak</a> üê¶.</p>
<p>If you would like to get monthly updates, consider <a href="https://sub.adipolak.com/subscribe">subscribing</a>.</p>
<h2 id="-learn-more">üí° Learn more!</h2>
<ul>
<li>Watch this video on how to <a href="https://www.youtube.com/watch?v=eOhAzjf__iQ">architect structured streaming</a>.</li>
<li>Read here about <a href="https://docs.microsoft.com/en-us/azure/databricks/getting-started/spark/streaming?WT.mc_id=delta-13569-adpolak">Azure Databricks and Streaming</a>.</li>
</ul>
<p>If you didn&rsquo;t get a chance to read the previous posts, read here: <br></p>
<ol>
<li><a href="https://blog.adipolak.com/post/delta-lake-essential-fundamentals/">Delta Lake essential Fundamentals: Part 1 - ACID</a></li>
<li><a href="https://blog.adipolak.com/post/delta-lake-essential-fundamentals-the-deltalog/">Delta Lake essential Fundamentals: Part 2 - The DeltaLog</a></li>
<li><a href="https://blog.adipolak.com/post/delta-lake-essential-fundamentals-part-3/">Delta Lake essential Fundamentals: Part 3 - Compaction and Checkpoint</a></li>
<li>Delta Lake essential Fundamentals: Part 4 - Practical Scenarios (You are here)</li>
</ol>
<ul class="pa0">
  
   <div class="list">
     <span href="/tags/open-source" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">open-source</span>
   </div>
  
   <div class="list">
     <span href="/tags/apache-spark" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">apache spark</span>
   </div>
  
   <div class="list">
     <span href="/tags/delta-lake" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">delta lake</span>
   </div>
  
   <div class="list">
     <span href="/tags/beginner" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">beginner</span>
   </div>
  
   <div class="list">
     <span href="/tags/scenarios" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">scenarios</span>
   </div>
  
</ul>
</div>
    </article>
  </div>
</div>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <span class="f4 fw4 white-70 dn dib-ns pv2 ph3">
    Made with ‚ù§Ô∏è on üåé.
    
  </span>  
  <div>










</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
